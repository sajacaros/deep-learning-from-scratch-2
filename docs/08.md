# CHAPTER 8 어텐션

## 8.1 어텐션의 구조
### 8.1.1 seq2seq의 문제점
* seq2seq의 문제점
  * Encoder는 긴 문장도 고정길이의 벡터로 변환
  * 필요한 정보가 벡터에 다 담기지 못함
  * ![seq2seq문제점](images/fig%208-1.png)
### 8.1.2 Encoder 개선
* Encoder 출력의 길이는 입력 문장의 길에 따라 바꿔주는게 좋음
  * ![hs](images/fig%208-2.png)
  * ![hs2](images/fig%208-3.png)
### 8.1.3 Decoder 개선1
* Decoder에 hs 전달
  * ![hs->Decoder](images/fig%208-4.png)
* hs를 적용한 기존 Decoder 계층
  * ![hs적용 기존 Decoder 계층](images/fig%208-5.png)
* 얼라인먼트
  * alignment 
  * 단어(혹은 문구)의 대응 관계를 나타내는 정보
* 어텐션이란?
  * 필요한 정보에만 주목하여 그 정보로부터 시계열 변환을 수행
  * '도착어 단어'와 대응관계에 있는 '출발어 단어'의 정보를 골라내고 그 정보를 이용하여 번역을 수행
  * ![Decoder 계층 구성](images/fig%208-6.png)
    * 각 단어에서 Decoder에 입력된 단어와 대응 관계인 단어의 벡터를 hs에서 골라내는 것이 목표
    * ![가중치](images/fig%208-7.png)
    * ![가중합](images/fig%208-8.png)
* 미니배치 처리용 가중합 구현
``` 
N, T, H = 10, 5, 4
hs = np.random.randn(N, T, H)
a = np.random.randn(N, T)
ar = a.reshape(N, T, 1).repeat(H, axis=2)

t = hs * ar
print(t.shape) # (10, 5, 4)

c = np.sum(t, axis=1)
print(c.shape) # (10, 4)
```
* 가중합 계산 그래프
  * ![가중합 계산그래프](images/fig%208-11.png)
``` 
class WeightSum:
    def __init__(self):
        self.params, self.grads = [], []
        self.cache = None
    
    def forward(self, hs, a):
        N, T, H = hs.shape
        
        ar = a.reshape(N, T, 1).repeat(H, axis=2)
        t = hs * ar
        c = np.sum(t, axit=1)
        
        self.cache = (hs, ar)
        return c
    
    def backward(self, dc):
        hs, ar = self.cache
        N, T, H = hs.shape
        
        dt = dc.reshape(N, 1, H).repeat(T, axis=1)
        dar = dt * hs
        dhs = dt * ar
        da = np.sum(dar,axis=2)
        
        return dhs, da
```
### 8.1.4 Decoder 개선2
* Decoder의 은닉상태 벡터
  * ![Decoder의 은닉상태 벡터](images/fig%208-12.png)
  * 내적을 통해 h가 hs의 각 단어 벡터와 얼마나 '비슷한가'를 수치로 나타내기
    * ![내적](images/fig%208-13.png)
    * ![Softmax를 통한 정규화](images/fig%208-14.png)
``` 
N, T, H = 10, 5, 4
hs = np.random.randn(N, T, H)
h = np.random.randn(N, H)
hr = h.reshape(N, 1, H).repeat(T, axis=1)

t = hs * hr
print(T.shape) # (10, 5, 4)

s = np.sum(t, axis=2)
print(s.shape) # (10, 5)

softmax = Softmax()
a = softmax.forward(s)
print(a.shape) # (10, 5)
```
* 가중치 계산 그래프
  * ![가중치 계산 그래프](images/fig%208-15.png)
``` 
class AttentionWeight:
    def __init__(self):
        self.params, self.grads = [], []
        self.softmax = Softmax()
        self.cache = None

    def forward(self, hs, h):
        N, T, H = hs.shape

        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)
        t = hs * hr
        s = np.sum(t, axis=2)
        a = self.softmax.forward(s)

        self.cache = (hs, hr)
        return a

    def backward(self, da):
        hs, hr = self.cache
        N, T, H = hs.shape

        ds = self.softmax.backward(da)
        dt = ds.reshape(N, T, 1).repeat(H, axis=2)
        dhs = dt * hr
        dhr = dt * hs
        dh = np.sum(dhr, axis=1)

        return dhs, dh
```
### 8.1.5 Decoder 개선3
* Weight Sum 계층(8.1.3)과 Attention Weight 계층(8.1.4) 합치기
  * ![합치기](images/fig%208-16.png)
  * ![Attention 계층 정리](images/fig%208-17.png)
``` 
class Attention:
    def __init__(self):
        self.params, self.grads = [], []
        self.attention_weight_layer = AttentionWeight()
        self.weight_sum_layer = WeightSum()
        self.attention_weight = None

    def forward(self, hs, h):
        a = self.attention_weight_layer.forward(hs, h)
        out = self.weight_sum_layer.forward(hs, a)
        self.attention_weight = a
        return out

    def backward(self, dout):
        dhs0, da = self.weight_sum_layer.backward(dout)
        dhs1, dh = self.attention_weight_layer.backward(da)
        dhs = dhs0 + dhs1
        return dhs, dh
```
* Attention 계층을 갖춘 Decoder의 계층 구성
  * ![Decoder with Attention](images/fig%208-18.png)
  * ![비교](images/fig%208-19.png)
* 다수의 Attention 계층을 Time Attention 계층을 모아 구현하기
  * ![Time Attention](images/fig%208-20.png)
``` 
class TimeAttention:
    def __init__(self):
        self.params, self.grads = [], []
        self.layers = None
        self.attention_weights = None

    def forward(self, hs_enc, hs_dec):
        N, T, H = hs_dec.shape
        out = np.empty_like(hs_dec)
        self.layers = []
        self.attention_weights = []

        for t in range(T):
            layer = Attention()
            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])
            self.layers.append(layer)
            self.attention_weights.append(layer.attention_weight)

        return out

    def backward(self, dout):
        N, T, H = dout.shape
        dhs_enc = 0
        dhs_dec = np.empty_like(dout)

        for t in range(T):
            layer = self.layers[t]
            dhs, dh = layer.backward(dout[:, t, :])
            dhs_enc += dhs
            dhs_dec[:,t,:] = dh

        return dhs_enc, dhs_dec
```

## 8.2 어텐션을 갖춘 seq2seq 구현
### 8.2.1 Encoder 구현
* Encoder 구현
``` 
class AttentionEncoder(Encoder):
    def forward(self, xs):
        xs = self.embed.forward(xs)
        hs = self.lstm.forward(xs)
        return hs

    def backward(self, dhs):
        dout = self.lstm.backward(dhs)
        dout = self.embed.backward(dout)
        return dout
```
### 8.2.2 Decoder 구현
* Decoder 계층 구성
  * ![Decoder 계층](images/fig%208-21.png)
  * [AttentionDecoder](../ch08/attention_seq2seq.py)
### 8.2.3 seq2seq 구현
* Encoder -> AttentionEncoder
* Decoder -> AttentionDecoder
``` 
class AttentionSeq2seq(Seq2seq):
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        args = vocab_size, wordvec_size, hidden_size
        self.encoder = AttentionEncoder(*args)
        self.decoder = AttentionDecoder(*args)
        self.softmax = TimeSoftmaxWithLoss()

        self.params = self.encoder.params + self.decoder.params
        self.grads = self.encoder.grads + self.decoder.grads
```

## 8.3 어텐션 평가
### 8.3.1 날짜 형식 변환 문제
* 날짜 형식
  * ![예제](images/fig%208-23.png)
### 8.3.2 어텐션을 갖춘 seq2seq의 학습
* 학습
``` 
model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)
optimizer = Adam()
trainer = Trainer(model, optimizer)

acc_list = []
for epoch in range(max_epoch):
    trainer.fit(x_train, t_train, max_epoch=1, batch_size=batch_size, max_grad=max_grad)
```
* 결과
    * ![결과](images/fig%208-24.png)
* seq2seq(7장), peeky, attention 비교 결과
  * ![비교](images/fig%208-26.png)
### 8.3.3 어텐션 시각화
* 가중치 시각화
  * [visualize_attention 코드](../ch08/visualize_attention.py)
  * ![시각화](images/fig%208-27.png)

## 8.4 어텐션에 관한 남은 이야기
### 8.4.1 양방향 RNN
* 지금까지 구현한 Encoder
  * ![Encoder](images/fig%208-29.png)
  * 왼쪽에서 오른쪽 방향으로 주변 정보를 담음
* 양방향 LSTM 인코딩 
  * 양방향 처리를 통해 오른쪽에서 왼쪽으로의 주변 정보를 담을 수 있음 
  * ![양방향 LSTM](images/fig%208-30.png)
  * 구현 방법
    * 2개의 LSTM 계층 사용
    * 입력 문장의 단어들을 반대 순서로 나열
### 8.4.2 Attention 계층 사용 방법
* Attention 계층의 위치 변경
  * ![위치 변경](images/fig%208-32.png)
  * 정확도는 실제 데이터를 이용해서 검증해야함  
### 8.4.3 seq2seq 심층화와 skip 연결
* 3층 LSTM 계층의 어텐션 seq2seq
  * ![3층 LSTM with Attention](images/fig%208-33.png)
  * Encoder와 Decoder는 같은 층수의 LSTM을 이용하는 것이 일반적
* skip 연결
  * skip connection
  * residual connection(잔차 연결)
  * short-cut
  * ![skip](images/fig%208-34.png)
    * skip 연결의 접속부에서는 2개의 출력이 더해져서 출력
    * 덧셈 연산은 역전파시 기울기 소실되지 않고 전파되어 좋은 학습 기대할 수 있음

## 8.5 어텐션 응용
### 8.5.1 구글 신경망 기계 번역(GNMT)
* [논문](https://arxiv.org/abs/1609.08144)
* ![GNMT 계층](images/fig%208-35.png)
  * Encoder와 Decoder
  * Attention
  * LSTM 계층의 다층화
  * 첫 계층의 양방향 LSTM
  * skip  연결
### 8.5.2 트랜스포머
* RNN은 이전 시각에 계산한 결과를 순성대로 계산하므로 시간 방향으로 병렬 계산이 불가능함
* 트랜스포머 모델이란?
  * 'RNN이 아닌' 어텐션을 사용해 처리
  * 어텐션으로 구성되는데, 그 중 셀프어텐션이라는 기술을 이용
  * ![셀프어텐션](images/fig%208-37.png)
    * Time Attention 계층에는 서로 다른 두 시계열 데이터가 입력
    * Self-Attention은 두 입력선이 하나의 시계열 데이터로부터 나옴
      * 하나의 시계열 데이터 내에서의 원소 간 대응 관계가 구해짐
  * ![트랜스포머의 계층 구성](images/fig%208-38.png)
    * RNN 대신 어텐션을 사용
### 8.5.3 뉴럴 튜링 머신(NTM)
* 뉴럴 튜링 머신
  * NTM; Neural Turing Machine
  * RNN의 외부에 정보 저장용 메모리 기능을 배치
  * 어텐션을 이용하여 메모리로부터 필요한 정보를 읽거나 쓰는 방법
  * ![계층 구성](images/fig%208-41.png)
    * Write Head 계층과 Read Head 계층이 메모리로부터 쓰기/읽기 작업 수행
    * 컴퓨터의 메모리 조작을 모방하기 위해서 2개의 어텐션 이용
      * 콘텐츠 기반 어텐션
        * 입력으로 주어진 벡터와 비슷한 벡터를 메모리로부터 찾는 용도
      * 위치 기반 어텐션
        * 이전 시각에 주목한 메모리의 위치를 기준으로 전후 이동하는 용도

## 8.6 정리
* 번역이나 음성 인식 등, 한 시계열 데이터를 다른 시계열 데이터로 변환하는 작업에서는 시계열 데이터 사이의 대응 관계가 존재하는 경우가 많다.
* 어텐션은 두 시계열 데이터 사이의 대응 관계를 데이터로부터 학습한다.
* 어텐션에서는 벡터의 내적을 사용해 벡터 사이의 유사도를 구하고, 그 유사도를 이용한 가중한 벡터가 어텐션의 출력이 된다.
* 어텐션에서 사용하는 연산은 미분 가능하기 때문에 오차역전파법으로 학습할 수 있다.
* 어텐션이 산출하는 가중치를 시각화하면 입출력의 대응 관계를 볼 수 있다.
* 외부 메모리를 활용한 신경망 확장 연구에서는 메모리를 읽고 쓰는데 어텐션을 사용했다.