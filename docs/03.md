# CHAPTER 3 word2vec

## 3.1 추론 기반 기법과 신경망
### 3.1.1 통계 기반 기법의 문제점
* 대규모 말뭉치를 다룰 때 문제가 발생함
  * ![통계 vs 추론](images/fig%203-1.png)
### 3.1.2 추론 기반 기법 개요
* 추론이란?
  * 주변 단어(맥락)가 주어졌을때 무슨 단어가 들어가는지를 추측하는 작업
  * ![추론](images/fig%203-2.png)
* 추론 기반 기법의 모델은 맥락 정보를 입력받아 각 단어의 출현 확률을 출력
  * ![추론기반기법](images/fig%203-3.png)
### 3.1.3 신경망에서의 단어 처리
* 원핫 표현
  * ex> ("you", "say", "goodbuy", "and", "i", "hello", ".") 
  * ![원핫 표현](images/fig%203-4.png)

## 3.2 단순한 word2vec
### 3.2.1 CBOW 모델의 추론 처리
* CBOW란?
  * continuous bag-of-words
  * 맥락으로부터 타깃을 추측하는 용도의 신경망
  * ![CBOW](images/fig%203-9.png)
  * 입력층이 2개인 이유는 맥락으로 고려한 단어를 2개로 가정했기 때문
  * 즉 2개의 단어가 주어지면 아웃풋은 가운데 들어올 단어를 출력한다.
  * ![CBOW 모델의 신경망](images/fig%203-11.png)
``` 
import numpy as np
from common.layers import MatMul

c0 = np.array([[1,0,0,0,0,0,0]])
c1 = np.array([[0,0,1,0,0,0,0]])

W_in = np.random.randn(7,3)
W_out = np.random.randn(3,7)

in_layer0 = MatMul(W_in)
in_layer1 = MatMul(W_in)
out_layer = MatMul(W_out)

h0 = in_layer0.forward(c0)
h1 = in_layer1.forward(c1)
h = 0.5 * (h0+h1)
s = out_layer.forward(h)

s = out_layer.forward(h)
print(s)
```
### 3.2.2 CBOW 모델의 학습
* ![CBOW 모델의 학습시 신경망](images/fig%203-14.png)
### 3.2.3 word2vec의 가중치와 분산 표현
* 입력 측 완전연결계층의 가중치(Win), 출력 측 완전연결계층의 가중치(Wout)
* Win이 각 단어의 분산 표현
  * ![분산 표현](images/fig%203-15.png)

## 3.3 학습 데이터 준비
### 3.3.1 맥락과 타깃
* 말뭉치에서 맥락과 타깃을 만드는 예
  * [맥락과 타깃 ex](images/fig%203-16.png)
  * [id 기준 맥락과 타깃](images/fig%203-17.png)
``` 
from common.util import preprocess

text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)

def create_contexts_target(corpus, window_size=1):
  target = corpus[window_size:-window_size]
  contexts = []
  
  for idx in range(window_size, len(corpus)-window_size):
    cs = []
    for t in range(-window_size, window_size + 1):
      if t==0:
        continue
      cs.append(corpus[idx+t])
    contexts.append(cs)
  return np.array(contexts), np.array(target)
  
contexts, target = create_contexts_target(corpus, window_size=1)
print(contexts)
print(target)
```
### 3.3.2 원핫 표현으로 변환
``` 
from common.util import convert_one_hot
vocab_size = len(word_to_id)
target = convert_one_hot(target, vocab_size)
contexts = convert_one_hot(contexts, vocab_size)
```

## 3.4 CBOW 모델 구현
* CBOW 모델의 신경망
  * ![CBOW 신경망](images/fig%203-19.png)
``` 
import numpy as np
from common.layers import MatMul, SoftmaxWithLoss

class SimpleCBOW:
  def __init__(self, vocab_size, hidden_size):
    V, H = vocab_size, hidden_size
    
    # 가중치 초기화
    W_in = 0.01 * np.random.randn(V, H).astype('f')
    W_out = 0.01 * np.random.randn(H, V).astype('f')
    
    # 계층 생성
    self.in_layer0 = MatMul(W_in)
    self.in_layer1 = MatMul(W_in)
    self.out_layer = MatMul(W_out)
    self.loss_layer = SoftmaxWithLoss()
    
    # 가중치와 기울기 모으기
    layers = [self.in_layer0, self.in_layer1, self.out_layer]
    self.params, self.grads = [], []
    for layer in layers:
      self.params += layer.params
      self.grads += layer.grads
    
    # 단어의 분산 표현 저장
    self.word_vecs = W_in
    
    # 순정파
  def forward(self, contexts, target):
    h0 = self.in_layer0.forward(contexts[:, 0])
    h1 = self.in_layer1.forward(contexts[:, 1])
    h = (h0 + h1) * 0.5
    score = self.out_layer.forward(h)
    loss = self.loss_layer.forward(score, target)
    return loss
  
  # 역전파
  def backward(self, dout=1):
    ds = self.loss_layer.backward(dout)
    da = self.out_layer.backward(ds)
    da *= 0.5
    print(type(da))
    self.in_layer1.backward(da)
    self.in_layer0.backward(da)
    return None
```
* 역전파 계산 그래프
  * ![역전파](images/fig%203-20.png)
### 3.4.1 학습 코드 구현
* 학습 구현
``` 
from common.trainer import Trainer
from common.optimizer import Adam

window_size = 1
hidden_size = 5
batch_size = 3
max_epoch = 1000

text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)

vocab_size = len(word_to_id)
contexts, target = create_contexts_target(corpus, window_size)
target = convert_one_hot(target, vocab_size)
contexts = convert_one_hot(contexts, vocab_size)

model = SimpleCBOW(vocab_size, hidden_size)
optimizer = Adam()
trainer = Trainer(model, optimizer)

trainer.fit(contexts, target, max_epoch, batch_size)
trainer.plot()

word_vecs = model.word_vecs
for word_id, word in id_to_word.items():
    print(word, word_vecs[word_id]) 
```
## 3.5 word2vec 보충
### 3.5.1 CBOW 모델과 확률
* CBOW 확률
  * ![CBOW 화률](images/e%203-1.png)
* CBOW 손실 함수
  * ![CBOW 손실 함수](images/e%203-3.png)
### 3.5.2 skip-gram 모델
* CBOW 모델과 skip-gram 모델이 다루는 문제
  * ![CBOW vs skip-gram](images/fig%203-23.png)
* skip-gram의 신경망
  * ![skip-gram의 신경망](images/fig%203-24.png)
* skip-gram 확률
  * ![skip-gram 확률](images/e%203-5.png)
* skip-gram 손실 함수
  * ![skip-gram 손실 함수](images/e%203-7.png)
* 단어 분산 표현의 정밀도 면에서 skip-gram 모델의 결과가 더 좋음
### 3.5.3 통계 기반 vs. 추론 기반
* 추론 기반과 통계 기반 기법의 우열을 가릴 수 없음
* GloVe 기법
  * 추론 기반 기법과 통계 기반 기법을 융합
  * 말뭉치 전체의 통계 정보를 손실 함수에 도입해 미니배치 학습을 하는 것

## 3.6 정리
* 추론 기반 기법은 추측하는 것이 목적이며, 그 부산물로 단어의 분산 표현을 얻을 수 있다.
* word2vec은 추론 기반 기법이며, 단순한 2층 신경망이다.
* word2vec은 skip-gram 모델과 CBOW 모델을 제공한다.
* CBOW 모델은 여러 단어(맥락)로부터 하나의 단어(타깃)을 추측한다.
* 반대로 skip-gram 모델은 하나의 단어(타깃)로부터 다수의 단어(맥락)를 추측한다.
* word2vec은 가중치를 다시 학습할 수 있으므로, 단어의 분산 표현 갱신이나 새로운 단어 추가를 효율적으로 수행할 수 있다.
