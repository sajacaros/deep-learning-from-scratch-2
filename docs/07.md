# CHAPTER 7 RNN을 사용한 문장 생성
* seq2seq 신경망 활용
  * 한 시계열 데이터를 다른 시계열 데이터로 변환
  * 기계 번역, 챗봇, 메일의 자동 답신 등 다양하게 응용

## 7.1 언어 모델을 사용한 문장 생성
### 7.1.1 RNN을 사용한 문장 생성의 순서
* LSTM 신경망
  * ![LSTM 신경망](images/fig%207-1.png)
* LSTM 확률분포
  * ![LSTM 확률분포](images/fig%207-2.png)
  * 결정적 선택
    * 확률이 가장 높은 단어 선택
  * 확률적 선택
    * 각 후보 단어의 확률에 맞추어 선택
    * 선택 되는 단어가 매번 다름
    * ![확률적 선택](images/fig%207-3.png)
    * ![반복적 확률적 선택](images/fig%207-4.png)
### 7.1.2 문장 생성 구현
* RnnlmGen 클래스 구현
  * 문장 생성 메서드 추가
``` 
class RnnlmGen(Rnnlm):
    def generate(self, start_id, skip_idx=None, sample_size=100):
        word_ids = [start_id]
        x = start_id
        while len(word_ids) < sample_size):
            x = np.array(x).reshape(1, 1)
            score = self.predict(x)
            p = softmax(score.flatten())
            
            sampled = np.random.choice(len(p), size=1, p=p)
            if (skip_ids is None) or (sampled not in skip_ids):
                x = sampled
                word_ids.append(int(x))
        return word_ids
```
### 7.1.3 더 좋은 문장으로
* RnnlmGen이 BetterRnnlm 클래스를 상속받으면 더 좋은 문장 생성

## 7.2 seq2seq
### 7.2.1 seq2seq의 원리
* seq2seq
  * sequence to sequence
  * Encoder-Decoder 모델
  * ![번역 예](images/fig%207-5.png)
  * Encoder와 Decoder가 협력하여 시계열 데이터를 다른 시계열 데이터로 변환
* Encoder
  * ![Encoder 계층](images/fig%207-6.png)
  * 임의 길이의 문장을 고정 길이 벡터로 변환
    * ![벡터로 인코딩](images/fig%207-7.png)
* Decoder
  * ![Decoder 계층](images/fig%207-8.png)
  * LSTM 신경망과 같은 구성
    * LSTM 계층이 벡터 h를 입력 받음
* seq2seq 전체 계층
  * ![전체 계층](images/fig%207-9.png)
  * Encoder의 LSTM과 Decoder의 LSTM으로 구성
  * 순전파
    * Encoder->Decoder로 인코딩된 정보가 전달
  * 역전파
    * Decoder->Encoder로 기울기가 전달
### 7.2.2 시계열 데이터 변환용 장난감 문제
* 덧셈 예제
  * ![덧셈 예제](images/fig%207-10.png)
### 7.2.3 가변 길이 시계열 데이터
* 패딩을 이용하여 동일 크기로 맞춤
  * ![동일 크기](images/fig%207-11.png)
### 7.2.4 덧셈 데이터셋
* dataset/addintion.txt에 담겨져 있음
  * ![예제](images/fig%207-12.png)

## 7.3 seq2seq 구현
### 7.3.1 Encoder 클래스
* Encoder 입출력
  * ![Encoder 입출력](images/fig%207-13.png)
* Encoder 계층 구성
  * ![Encoder 계층 구성](images/fig%207-14.png)
  * Encoder에서 h를 출력하고 이 h는 Decoder로 전달
``` 
class Encoder:
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        V, D, H = vocat_size, wordvec_size, hidden_size
        rn = np.random.randn
        
        embed_W = (rn(V, D) / 100).astype('f')
        lstm_Wx = (rn(D, 4*H) / np.sqrt(D)).astype('f')
        lstm_Wh = (rn(H, 4*H) / np.sqrt(H)).astype('f')
        lstm_b = np.zeros(4*H).astype('f')
        
        self.embed = TimeEmbedding(embed_W)
        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, statefull=False)
        self.params = self.embed.params + self.lstm.params
        self.grads = self.embed.grads + self.lstm.grads
        self.hs = None
        
    def forward(self, ws):
        xs = self.embed.forward(xs)
        hs = self.lstm.forward(xs)
        self.hs = hs
        return hs[:, -1, :]
    
    def backward(self, dh):
        dhs = np.zeros_like(self.hs)
        dhs[:, -1, :] = dh
        
        dout = self.lstm.backward(dhs)
        dout = self.embed.backward(dout)
        
        return dout
```
### 7.3.2 Decoder 클래스
* Decoder 계층 구성
  * ![Decoder 계층](images/fig%207-17.png)
  * 확률 대신 결적적인 답 도출
    * argmax 노드 도입 
    * ![decoder](images/fig%207-18.png)
* Decoder 클래스 구성
  * 학습 시와 생성 시에 Softmax 계층을 다르게 취급
  * ![Decoder 클래스](images/fig%207-19.png)
``` 
class Decoder:
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        V, D, H = vocab_size, wordvec_size, hidden_size
        rn = np.random.randn

        embed_W = (rn(V, D) / 100).astype('f')
        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b = np.zeros(4 * H).astype('f')
        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')
        affine_b = np.zeros(V).astype('f')

        self.embed = TimeEmbedding(embed_W)
        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)
        self.affine = TimeAffine(affine_W, affine_b)

        self.params, self.grads = [], []
        for layer in (self.embed, self.lstm, self.affine):
            self.params += layer.params
            self.grads += layer.grads

    def forward(self, xs, h):
        self.lstm.set_state(h)

        out = self.embed.forward(xs)
        out = self.lstm.forward(out)
        score = self.affine.forward(out)
        return score

    def backward(self, dscore):
        dout = self.affine.backward(dscore)
        dout = self.lstm.backward(dout)
        dout = self.embed.backward(dout)
        dh = self.lstm.dh
        return dh
```
* generate()
  * argmax를 통한 문자 생성
``` 
    def generate(self, h, start_id, sample_size):
        sampled = []
        sample_id = start_id
        self.lstm.set_state(h)
      
        for _ in range(sample_size):
            x = np.array(sample_id).reshape((1, 1))
            out = self.embed.forward(x)
            out = self.lstm.forward(out)
            score = self.affine.forward(out)
      
            sample_id = np.argmax(score.flatten())
            sampled.append(int(sample_id))
      
        return sampled
```
### 7.3.3 Seq2seq 클래스
* Seq2seq 클래스
  * Encoder 클래스와 Decoder 클래스 연결
``` 
class Seq2seq(BaseModel):
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        V, D, H = vocab_size, wordvec_size, hidden_size
        self.encoder = Encoder(V, D, H)
        self.decoder = Decoder(V, D, H)
        self.softmax = TimeSoftmaxWithLoss()

        self.params = self.encoder.params + self.decoder.params
        self.grads = self.encoder.grads + self.decoder.grads

    def forward(self, xs, ts):
        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]

        h = self.encoder.forward(xs)
        score = self.decoder.forward(decoder_xs, h)
        loss = self.softmax.forward(score, decoder_ts)
        return loss

    def backward(self, dout=1):
        dout = self.softmax.backward(dout)
        dh = self.decoder.backward(dout)
        dout = self.encoder.backward(dh)
        return dout

    def generate(self, xs, start_id, sample_size):
        h = self.encoder.forward(xs)
        sampled = self.decoder.generate(h, start_id, sample_size)
        return sampled
```
### 7.3.4 seq2seq 평가
* 학습 흐름
  1. 학습 데이터에서 미니배치 선택
  2. 미니배치로부터 기울기 계싼
  3. 기울기를 사용하여 매개변수 갱신
``` 
# 일반 혹은 엿보기(Peeky) 설정 =====================================
model = Seq2seq(vocab_size, wordvec_size, hidden_size)
# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)
# ================================================================
optimizer = Adam()
trainer = Trainer(model, optimizer)

acc_list = []
for epoch in range(max_epoch):
    trainer.fit(x_train, t_train, max_epoch=1,
                batch_size=batch_size, max_grad=max_grad)

    correct_num = 0
    for i in range(len(x_test)):
        question, correct = x_test[[i]], t_test[[i]]
        verbose = i < 10
        correct_num += eval_seq2seq(model, question, correct,
                                    id_to_char, verbose, is_reverse)

    acc = float(correct_num) / len(x_test)
    acc_list.append(acc)
    print('검증 정확도 %.3f%%' % (acc * 100))
```

## 7.4 seq2seq 개선
* 속도 개선이 목표
### 7.4.1 입력 데이터 반전(Reverse)
* 입력 데이터의 순서 반번
  * ![데이터 반전](images/fig%207-23.png)
``` 
(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt')
x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]
```
* 개선 효과
  * ![결과](images/fig%207-24.png)
  * 기울기 전파가 원활해지기 때문
### 7.4.2 엿보기(Peeky)
* Peeky
  * Encoder의 출력 h를 Decoder의 다른 계층에서도 전해주기
  * ![Peeky](images/fig%207-26.png)
  * 계산그래프
    * ![계산 그래프](images/fig%207-27.png)
* 개선 효과
  * ![결과](images/fig%207-28.png)
  
## 7.5 seq2seq를 이용하는 애플리케이션
### 7.5.1 챗봇
* [Chat GPT](https://openai.com/gpt-4)
* [Llama 2](https://ai.meta.com/llama)
* [PaLM 2](https://ai.google/discover/palm2/)

## 7.6 정리
* RNN을 이용한 언어 모델은 새로운 문장을 생성할 수 있다.
* 문장을 생성할 때는 하나의 단어(혹은 문자)를 주고 모델의 출력(확률분포)에서 샘플링하는 과정을 반복한다.
* RNN을 2개 조합함으로써 시계열 데이터를 다른 시계열 데이터로 변환할 수 있다.
* seq2seq는 Encoder가 출발어 입력문을 인코딩하고, 인코딩된 정보를 Decoder가 받아 디코딩하여 도착어 출력문을 얻는다.
* 입력문을 반전시키는 기법(Reverse), 또는 인코딩된 정보를 Decoder의 여러 계층에 전달하는 기법(Peeky)은 seq2seq의 정확도 향상에 효과적이다.
* 기계 번역, 챗봇, 이미지 캡셔닝 등 seq2seq는 다양한 애플리케이션에 이용할 수 있다.
