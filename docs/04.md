# CHAPTER 4 word2vec 속도 개선
* Embedding 계층 도입
* 네거티브 샘플링 도입

## 4.1 word2vec 개선
* 원핫 표현의 벡터 크기
  * Embedding 계층 도입
* 은닉층과 가중치 행렬 Wout의 계산량과 Softmax의 계산량
  * 네거티브 샘플링 도입
### 4.1.1 Embedding 계층
* MatMul 계층 대신 매개변수로부터 '단어 ID에 해당하는 행(뻭터)'을 추출하는 계층
* 단어 임베딩(분산 표현) 저장
  * 단어의 밀집벡터 표현 
  * 통계 기반 기법으로 얻은 단어 임베딩 = distributional representation
  * 신경망을 사용한 추론 기법으로 얻은 단어 임베딩 = distributed representation
### 4.1.2 Embedding 계층 구현
* Embedding 계층의 forward와 backward
  * ![Embdding 계층의 forward와 backward](images/fig%204-4.png)
  * 순전파는 가중치 W의 특정행을 추출하여 전달
  * 역전파는 앞 층(출력층)으로부터 전해진 기울기를 다음 층(입력층)으로 전달
  * 역전파의 경우 겹치는 문제가 발생
    * ![역전파 겹침](images/fig%204-5.png)
    * 역전파에서 겹치는 부분은 기울기를 더해줌으로써 해결(가중치 강조 효과)
``` 
class Embedding
    def __init__(self, W):
        self.params = [W]
        self.grads = [np.zeros_like(W)]
        self.idx = None
    
    def forward(self, idx):
        W, = self.params
        self.idx = idx
        out = W[idx]
        return out
    
    def backward(self, out):
        dW, = self.grads
        dW[...] = 0
        
        for i, word_id in enumerate(self.idx):
            dW[wrod_id] += dout[i]
        return None
```

## 4.2 word2vec 개선
### 4.2.1 은닉층 이후 계산의 문제점
* word2vec 수행 작업
  * ![word2vec](images/fig%204-6.png)
  * 병목 계층
    * 은닉층의 뉴런과 가중치 행렬(Wout)의 곱
    * Softmax 계층의 계산
### 4.2.2 다중 분류에서 이진 분류로
* 네이티브 샘플링 기법
  * 다중 분류를 이진 분류로 근사하는 것
  * ex> '맥락이 you와 goodbye 일 때, 타깃 단어는 say입니까?'
  * ![타깃 단어만의 점수를 구하는 신경망](images/fig%204-7.png)
    * Wout 벡터에서 say에 해당하는 벡터만 추출하여 계산
    * ![say추츨 예제](images/fig%204-8.png)
    * h(1x100) x say's Wout(100x1) = S(1x1)
### 4.2.3 시그모이드 함수와 교차 엔트로피 오차
* 다중 분류
  * 츨력층에 소프트맥스 함수 사용
  * 손실함수에 교차 엔트로피 오차 사용
    * ![다중 분류 교차 엔트로피](images/e%201-7.png)
* 이진 분류
  * 출력층에 시그모이드 함수 사용
    * ![시그모이드](images/e%204-2.png)
  * 손실함수에 교차 엔트로피 오차 사용
    * ![이진 분류 교차 엔트로피](images/e%204-3.png)
### 4.2.4 다중 분류에서 이진 분류로 (구현)
* 다중 분류를 수행하는 word2vec 전체 그림
  * ![다중 분류](images/fig%204-11.png)
* 이진 분류를 수행하는 word2vec 전체 그림
  * ![이진 분류](images/fig%204-12.png)]
``` 
class EmbeddingDot:
    def __init__(self, W):
        self.embed = Embedding(W)
        self.params = self.embed.params
        self.grads = self.embed.grads
        self.cache = None
        
    def forward(self, h, idx):
        target_W = self.embed.forward(idx)
        out = np.sum(target_W * h, axis=1)
        
        self.cache = (h, target_W)
        return out
    
    def backward(self, dout):
        h, target_W = self.cache
        dout = dout.reshape(dout.shape[0], 1)
        
        dtarget_W = dout * h
        self.embed.backward(dtarget_W)
        dh = dout * target_W
        return dh
```
* Embedding Dot 계층의 각 변수의 구체적인 값
  * ![Embedding Dot 변수값](images/fig%204-14.png)
### 4.2.5 네거티브 샘플링
* 부정적인 예에 대해서도 학습이 이루어 져야함
  * ![금정적인예 vs 부정적인예](images/fig%204-16.png)
* 네거티브 샘플링 기법
  * 긍정적 예에 대한 손실과 부정적 예에 대한 손실을 합산하여 최종 손실로 처리
  * 부정적 예는 몇개를 샘플링하여 학습에 사용함
  * ![네거티브 샘플링](images/fig%204-17.png)
### 4.2.6 네거티브 샘플링의 샘플링 기법
* 샘플링 방법
  * 말뭉치에서 자주 등장하는 단어를 많이 추출하고 드물게 등장하는 단어를 적게 추출
  * numpy의 random.choice()메소드의 인수 p(확률 분포 리스트) 활용
  * 확률 분포에 0.75를 제곱하여 사용하도록 권고함(이론적 의미는 없음)
    * ![확률 분포](images/e%204-4.png)
### 4.2.7 네거티브 샘플링 구현
``` 
class NegativeSamplingLoss:
    def __init__(Self, W, corpus, power=0.75, sample_size=5):
        self.smaple_size = sample_size
        self.sampler = UnigramSampler(corpus, power, sample_size)
        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]
        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_Size + 1)]
        self.params, self.grads = [], []
        for layer in self.embed_dot_layers:
            self.params += layer.params
            slef.grads += layer.grads
    
    def forward(self, h, target):
        batch_size = target.shape[0]
        negative_sample = self.sampler.get_negative_Sample(target)
        
        # 긍정적 예 순전파
        score = self.embed_dot_layers[0].forward(h, target)
        correct_label = np.ones(batch_size, dtype=np.int32)
        loss = self.loss_layers[0].forward(score, correct_label)
        
        # 부정적 예 순전파
        negative_label = np.zeros(batch_size, dtype=np.int32)
        for i in range(slef.sample_size):
            negative_target = negative_sample[:, i]
            score = self.embed_dot_layers[i+1].forward(h, negative_target)
            loss += self.loss_layers[i+1].forward(score, negative_label)
            
        return loss
    
    def backward(self, dout=1):
        dh = 0
        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):
            dscore = l0.backward(dout)
            dh += l1.backward(dscore)
        return dh
```

## 4.3 개선판 word2vec 학습
### 4.3.1 CBOW 모델 구현
``` 
class CBOW:
    # 어휘수, 은닉층 뉴런수, 문맥 크기, 단어 ID 목록
    def __init__(self, vocab_size, hidden_size, widnow_size, corpus):
        V, H = vocab_szie, hidden_size
        
        # 가중치 초기화
        W_in = 0.01 * np.random.randn(V,H).astype('f')
        W_out = 0.01 * np.random.randn(V,H).astype('f)
        
        # 계층 생성
        self.in_layers = []
        for i in range(2*widnow_size):
            layer = Embedding(W_in)
            self.in_layers.append(layer)
        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)
        
        # 모든 가중치와 기울기를 배열에 모은다.
        layers = self.in_layers + [self.ns_loss]
        self.params, self.grads = [], []
        for layer in layers:
            self.params += layer.params
            self.grads += layer.grads
        
        # 인스턴스 변수에 단어의 분산 표현을 저장
        self.word_vecs = W_in
    
    def forward(self, contexts, target):
        h = 0
        for i, layer in enumerate(self.in_layers):
            h += layer.forward(contexts[:, i])
        h *= 1 / len(self.in_layers)
        loss = self.ns_loss.forward(h, target)
        return loss
        
    def backward(self, dout=1):
        dout = self.ns_loss.backward(dout)
        dout *= 1 / len(self.in_layers)
        for layer in self.in_layers:
            layer.backward(dout)
        return None
```
### 4.3.2 CBOW 모델 학습 코드
``` 
# 하이퍼파라미터 설정
window_size = 5
hidden_size = 100
batch_size = 100
max_epoch = 10

# 데이터 읽기
corpus, word_to_id, id_to_word = ptb.load_data('train')
vocab_size = len(word_to_id)

contexts, target = create_contexts_target(corpus, window_size)

# 모델 생성
model = CBOW(vocab_size, hidden_size, window_size, corpus)
optimizer = Adam()
trainer = Trainer(model, optimizer)

# 학습 시작
trainer.fit(contexts, target, max_epoch, batch_szie)
trainer.plot()

# 나중에 사용할 수 있도록 필요한 데이터 저장
word_vecs = model.word_vecs
params = {}
parmas['word_vecs'] = word_vecs.astype(np.float16)
params['word_to_id'] = word_to_id
params['id_to_word'] = id_to_word
pkl_file = 'cbow_params.pkl'
with open(pkl_file, 'wb') as f:
    pickle.dump(params, f, -1)
```
### 4.3.3 CBOW 모델 평가
``` 
pkl_file = 'cbow_params.pkl'
with open(pkl_file, 'rb') as f:
    params = pickle.load(f)
    word_vecs = params['word_vecs']
    word_to_id = params['word_to_id']
    id_to_word = params['id_to_word']
querys = ['you', 'year', 'car', 'toyota']
for query in querys:
    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)
```

## 4.4 word2vec 남은 주제
### 4.4.1 word2vec을 사용한 애플리케이션의 예
* 전이 학습
  * transfer learning
  * 한 분에서 배운 지식을 다른 분야에도 적용하는 기법
  * 큰 말뭉치로 학습을 끝난 후, 그 분산 표현을 각자의 작업에 이용
* 단어나 문장의 분산 표현을 사용하여 고정 길이 벡터로 변환할 수 있음
  * bag-of-words
    * * 문장의 각 단어를 분산 표현으로 변환하고 그 합을 구하는 것
  * 순환신경망(RNN)을 사용하면 문장을 고정 길이 벡터로 변환할 수 있음
  * 고정 길이 벡터는 일반적인 머신러닝 기법을 적용할 수 있음
  * ![단어의 분산 표현](images/fig%204-21.png)
  * ![단어의 분산 표현](images/fig%204-22.png)
### 4.4.2 단어 벡터 평가 방법
* 단어의 분산 표현의 우수성을 실제 애플리케이션과는 분리해 평가
* 분산 표현의 평가 척도
  * 단어의 '유사성'
  * '유추 문제'를 활용한 평가
    * ![유추문제에 의한 단어 벡터의 평가 결과](images/fig%204-23.png)
    * 모델에 따라 정확도가 다름
    * 말충치가 클수록 결과가 좋음
    * 단어 벡터 차원수는 적당한 크기가 좋음
    
## 4.5 정리
* Embedding 계층은 단어의 분산 표현을 담고 있으며, 순전파 시 지정한 단어 ID의 벡터를 추출한다.
* word2vec은 어휘 수의 증가에 비례하여 계산량도 증가하므로, 근사치로 계싼하는 빠른 기법을 사용하면 좋다.
* 네거티브 샘플링은 부정적 예를 몇 개 샘플링하는 기법으로, 이를 이용하면 다중 분류를 이진 분류처럼 위급할 수 있다.
* word2vec으로 얻은 단어의 분산 표현에는 단어의 의미가 녹아들어 있으며, 비슷한 맥락에서 사용되는 단어는 단어 벡터 공간에서 가까이 위치한다.
* word2vec의 단어의 분산 표현을 이용하면 유추 문제를 벡터의 덧셈과 뺄셈으로 풀 수 있게 된다.
* word2vec은 전이 학습 측면에서 특히 중요하며, 그 단어의 분산 표현은 다양한 자연어 처리 작업이 이용할 수 있다.