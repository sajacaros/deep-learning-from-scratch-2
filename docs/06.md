# CHAPTER 6 게이트가 추가된 RNN

## 6.1 RNN의 문제점
### 6.1.1 RNN 복습
* RNN 계층
  * ![RNN 계층](images/fig%206-1.png)
* RNN 계산 그래프
  * ![계산 그래프](images/fig%206-2.png)
### 6.1.2 기울기 소실 또는 기울기 폭발
* RNN 기울기 흐름
  * ![기울기 흐름](images/fig%206-4.png)
  * 기울기가 중간에 사그라들면서 가중치 매개변수가 갱신되지 않음
    * 장기 의존 관계를 학습할 수 없음
### 6.1.3 기울기 소실과 기울기 폭발의 원인
* 역전파에서는 기울기가 tanh 노드를 지날 때마다 값이 계속 작아짐
  * ![tanh](images/fig%206-6.png)
### 6.1.4 기울기 폭발 대책
* 기울기 폭발의 대책
  * 기울기 클리핑
    * gradients clipping
    * 기울기의 L2노름이 threshold 를 초과하면 기울기를 수정

## 6.2 기울기 소실과 LSTM
### 6.2.1 LSTM의 인터페이스
* RNN 계층과 LSTM 계층 비교
  * ![RNN vs LSTM](images/fig%206-11.png)
### 6.2.2 LSTM의 계층 조립하기
* c와 h를 계산하는 LSTM 계층
  * ![c,h 계산](images/fig%206-12.png)
* 게이트의 열림 상태를 학습합으로써 다음 계층으로 데이터를 얼마나 보낼지 결정함 
### 6.2.3 output 게이트
* output 게이트
  * 다음 은닉 상태 h의 출력을 담당하는 게이트
  * ![output 게이트 도식](images/e%206-1.png)
  * ![output 게이트 추가](images/fig%206-15.png)
  * o와 tanh()는 아다마르 곱(Hadamard product) 이용
    * 원소별 곱(RNN과의 차이점)
    * ![원소별 곱](images/e%206-2.png)
### 6.2.4 forget 게이트
* forget 게이트
  * 불필요한 기억을 잊게 해주는 게이트
  * ![forget 게이트](images/fig%206-16.png)
  * ![forget 게이트 도식](images/e%206-3.png)
### 6.2.5 새로운 기억 셀
* 새로운 기억 셀
  * 새로 기억해야 할 정보를 기억셀에 추가
  * ![새로운 기억 셀](images/fig%206-17.png)
  * ![도식](images/e%206-4.png)
### 6.2.6 input 게이트
* input 게이트
  * g에 게이트를 추가
  * 새로 추가되는 정보의 가치가 얼마나 큰지를 판단
  * ![input 게이트](images/fig%206-18.png)
  * ![input 게이트 식](images/e%206-5.png)
### 6.2.7 LSTM의 기울기 흐름
* 기울기 흐름
  * ![기울기 흐름](images/fig%206-19.png)
  * '+' 노드는 상류에서 전해지는 기울기를 그대로 전달
    * 기울기 감소 x
  * 'x' 노드는 행렬 곱이 아닌 원소별 곱(아마다르 곱)을 계산
    * 곱셈의 효과가 누적되지 않아 기울기 소실 x

## 6.3 LSTM 구현
* LSTM 수식 정리
  * ![gate](images/e%206-6.png)
  * ![c](images/e%206-7.png)
  * ![h](images/e%206-8.png)
* 각 식의 가중치를 모아서 계산
  * ![가중치 모으기](images/fig%206-20.png)
  * ![계산그래프](images/fig%206-21.png)
    * 모아진 게이트 가중치는 slice를 통해 복원 가능하다.
``` 
class LSTM:
  def __init__(self, Wx, Wh, b):
    self.params = [Wx, Wh, b]
    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
    self.cache = None
  
  def forward(self, x, h_prev, c_prev):
    Wx, Wh, b = self.params
    N, H = h_prev.shape
    
    A = np.matmul(x, Wx) + np.matmul(h_prev, Wh) + b
    
    # slice
    f = A[:, :H]
    g = A[:, H: 2*H]
    i = A[:, 2*H: 3H]
    o = A[:, 3*H: ]
    
    f = sigmoid(f)
    g = np.tanh(g)
    i = sigmoid(i)
    o = sigmoid(o)
    
    c_next = f * c_prev + g * i
    h_next = o * np.tanh(c_next)
    
    self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)
    return h_next, c_next
```
* 형상 추이
  * ![형상 추이](images/fig%206-22.png)
* slice 노드 처리
  * ![slice 노드 처리](images/fig%206-23.png)
### 6.3.1 Time LSTM 구현
* Time LSTM
  * T개분의 시계열 데이터를 한꺼번에 처리하는 계층
  * ![Time LSTM](images/fig%206-24.png)
``` 
class TimeLSTM:
    def __init__(self, Wx, Wh, b, stateful=False):
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.layers = None

        self.h, self.c = None, None
        self.dh = None
        self.stateful = stateful

    def forward(self, xs):
        Wx, Wh, b = self.params
        N, T, D = xs.shape
        H = Wh.shape[0]

        self.layers = []
        hs = np.empty((N, T, H), dtype='f')

        if not self.stateful or self.h is None:
            self.h = np.zeros((N, H), dtype='f')
        if not self.stateful or self.c is None:
            self.c = np.zeros((N, H), dtype='f')

        for t in range(T):
            layer = LSTM(*self.params)
            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)
            hs[:, t, :] = self.h

            self.layers.append(layer)

        return hs

    def backward(self, dhs):
        Wx, Wh, b = self.params
        N, T, H = dhs.shape
        D = Wx.shape[0]

        dxs = np.empty((N, T, D), dtype='f')
        dh, dc = 0, 0

        grads = [0, 0, 0]
        for t in reversed(range(T)):
            layer = self.layers[t]
            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)
            dxs[:, t, :] = dx
            for i, grad in enumerate(layer.grads):
                grads[i] += grad

        for i, grad in enumerate(grads):
            self.grads[i][...] = grad
        self.dh = dh
        return dxs

    def set_state(self, h, c=None):
        self.h, self.c = h, c

    def reset_state(self):
        self.h, self.c = None, None
```

## 6.4 LSTM을 사용한 언어 모델
* LSTM을 이용한 모델
  * ![모델](images/fig%206-26.png)
``` 
class Rnnlm(BaseModel):
    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):
        V, D, H = vocab_size, wordvec_size, hidden_size
        rn = np.random.randn

        # 가중치 초기화
        embed_W = (rn(V, D) / 100).astype('f')
        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b = np.zeros(4 * H).astype('f')
        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')
        affine_b = np.zeros(V).astype('f')

        # 계층 생성
        self.layers = [
            TimeEmbedding(embed_W),
            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),
            TimeAffine(affine_W, affine_b)
        ]
        self.loss_layer = TimeSoftmaxWithLoss()
        self.lstm_layer = self.layers[1]

        # 모든 가중치와 기울기를 리스트에 모은다.
        self.params, self.grads = [], []
        for layer in self.layers:
            self.params += layer.params
            self.grads += layer.grads

    def predict(self, xs):
        for layer in self.layers:
            xs = layer.forward(xs)
        return xs

    def forward(self, xs, ts):
        score = self.predict(xs)
        loss = self.loss_layer.forward(score, ts)
        return loss

    def backward(self, dout=1):
        dout = self.loss_layer.backward(dout)
        for layer in reversed(self.layers):
            dout = layer.backward(dout)
        return dout

    def reset_state(self):
        self.lstm_layer.reset_state()
```

## 6.5 RNNLM 추가 개선
### 6.5.1 LSTM 계층 다층화
* LSTM계층을 2층으 쌓은 RNNLM
  * ![2층](images/fig%206-29.png)
  * PTB 데이터셋의 경우 2~4층이 좋은 결과 보임
### 6.5.2 드롭아웃에 의한 과적합 억제
* RNN은 일반적인 피드포워드 신경망 보다 쉽게 과적합을 일으킴
  * ![LSTM의 드롭아웃](images/fig%206-33.png)
  * 변형 드롭아웃
    * 같은 계층에 속한 드롭아웃들은 같은 마스크를 공유
    * 마스크란 데이터의 통과/차단을 결정하는 이진 형태의 무작위 패턴
    * ![마스크 드롭아웃](images/fig%206-34.png)
### 6.5.3 가중치 공유
* 가중치 공유
  * Embedding 계층의 가중치와 Affine 계층의 가중치를 공유하는 기법
  * ![가중치 공유](images/fig%206-35.png)
### 6.5.4 개선된 RNNLM 구현
* 개선된 RNNLM 신경망
  * ![개선된 RNNLM](images/fig%206-36.png)
### 6.5.5 첨단 연구로
* 다층 LSTM 사용
* 드롭아웃 기반의 정규화 수행
* 가중치 공유 수행

## 6.6 정리
* 단순한 RNN의 학습에서는 기울기 소실과 기울기 폭발이 문제가 된다.
* 기울기 폭발에는 기울기 클리핑, 기울기 소실에는 게이트가 추가된 RNN이 효과적이다.
* LSTM에는 input 게이트, forget 게이트, output 게이트 등 3개의 게이트가 있다.
* 게이트에는 전용 가중치가 있으며, 시그모이드 함수를 사용하여 0.0~1.0 사이의 실수를 출력한다.
* 언어 모델 개선에는 LSTM 계층 다층화, 드롭아웃, 가중치 공유 들의 기법이 효과적이다.
* RNN의 정규화는 중요한 주제이며, 드롭아웃 기반의 다양한 기법이 제안되고 있다.