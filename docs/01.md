# CHAPTER 1 신경망 복습

## 1.1 수학과 파이썬 복습
### 1.1.1 백터와 행렬
* 텐서란?
  * 백터와 행렬을 확장하여 숫자 집합을 N차원으로 표현한 것
### 1.1.2 행렬의 원소별 연산
* 다차원 넘파이 배열의 연산은 서로 대응하는 원소끼리 연산이 이뤄짐
``` 
W = np.array([[1,2,3],[4,5,6]])
X = np.array([[0,1,2],[3,4,5]])
W + X # [[1,3,5],[7,9,11]]
W * X # [[0,2,6],[12,20,30]]
```
### 1.1.3 브로드캐스트
* ![브로드캐스트 예1](images/fig%201-3.png)
* ![브로드캐스트 예1](images/fig%201-4.png)
### 1.1.4 백터의 내적과 형렬의 곱
* 백터의 내적
  * 두 벡터에서 대응하는 원소들의 곱을 모두 더한 것
* 행렬의 곱
  * ![행렬의 곱](images/fig%201-5.png)
* 벡터의 내적과 행렬의 곱 모두 np.dot을 이용해 계산 가능
``` 
# 내적
a = np.array([1,2,3])
b = np.array([4,5,6])
np.dot(a,b) # 32

# 행렬의 곱
a = np.array([[1,2],[3,4]])
b = np.array([[5,6],[7,8]])
np.dot(a,b)
# array([[19, 22],
#        [43, 50]])
```
### 1.1.5 행렬 형상 확인
* ![행렬 형상](images/fig%201-6.png)

## 1.2 신경망의 추론
### 1.2.1 신경망 추론 전체 그림
* 입력층 뉴런 2개, 출력층 뉴런 3개, 은닉층 뉴런 4개인 신경망
  * ![신경망 예제](images/fig%201-7.png)
  * 화살표 함수에는 가중치가 존재
### 1.2.2 계층으로 클래스화 및 순전파 구현
* 구현 규칙
  * 모든 계층은 forward()와 backward() 메서드를 가진다.
  * 모든 계층은 인스턴스 변수인 prams와 grads를 가진다.
* Sigmoid 계층
``` 
class Sigmoid:
  def __init__(self):
    self.params = []
  
  def forward(self, x):
    return 1 / (1+np.exp(-x))
```
* Affine 계층
``` 
class Affine:
  def __init__(self, W, b):
    self.params = [W, b]
    
  def forward(self, x):
    w, b = self.params
    out = np.matmul(x, W) + b
    return out
```
* 구현해볼 신경망 계층
  * ![구현예제](images/fig%201-11.png)
``` 
class TwoLayerNet:
  def __init__(self, input_size, hidden_size, output_size):
    I, H, O = input_size, hidden_size, output_size
    
    W1 = np.random.randn(I, H)
    b1 = np.random.randn(H)
    W2 = np.random.randn(H, O)
    b2 = np.random.randn(O)
    
    self.layer = [
      Affine(W1, b1),
      Sigmoid(),
      Affine(W2, b2)
    ]
    
    self.parmas = []
    for layer in self.layers:
      self.params += layer.params
  
  def predict(self, x):
    for layer in self.layers:
      x = layer.forward(x)
    return x
```

## 1.3 신경만의 학습
### 1.3.1 손실 함수
* 손실이란?
  * 학습 데이터와 신경망이 예측한 결과를 비교한 값
  * 다중 클래스 분류 
    * 교차 엔트로피 오차를 이용
* 손실함수를 추가한 신경망의 계층
  * ![손실함수](images/fig%201-13.png)
### 1.3.2 미분과 기울기
* 신경망 학습의 목표는 손실을 최소화하는 매개변수를 찾는 것
  * 미분과 기율기를 활용하여 학습을 진행
### 1.3.3 연쇄 법칙
* 신경망의 기율기는 오차역전파법을 사용하여 구현
  * 연쇄법칙
    * 합성함수에 대한 미분의 법칙
### 1.3.4 계산 그래프
* 계산 그래프 예제
  * ![계산 그래프 예제](images/fig%201-17.png)
* 곱셈 노드
  * ![곱셈 노드](images/fig%201-19.png)
* 분기 노드
  * ![분기 노드](images/fig%201-20.png)
* Repeat 노드
  * 2개로 분기하는 분기 노드를 일반화하여 N개로 분기
  * ![Repeat 노드](images/fig%201-21.png)
* Sum 노드
  * ![Sum 노드](images/fig%201-22.png)
  * Sum 노드의 역전파는 Repeat 노드로 구현
* MatMul 노드
  * ![MatMul 노드의 역전파](images/fig%201-25.png)
``` 
class MatMul:
  def __init__(self, W):
    self.params = [W]
    self.grads = [np.zeros_like(W)]
    self.x = None

  def forward(self, x):
    W, = self.params
    out = np.dot(x, W)
    self.x = x
    return out

  def backward(self, dout):
    W, = self.params
    dx = np.dot(dout, W.T)
    dW = np.dot(self.x.T, dout)
    self.grads[0][...] = dW
    return dx
```
### 1.3.5 기울기 도출과 역전파 구현
* Sigmoid 계층
``` 
class Sigmoid:
  def __init__(self):
    self.params, self.grads = [], []
    self.out = None
  def forward(self, x):
    out = 1 / (1 + np.exp(-x))
    self.out = out
    return out
  def backward(self, dout):
    dx = dout * (1.0 - self.out) * self.out
    reutnr dx
```
* Affine 계층
  * ![Affine 계층](images/fig%201-29.png)
``` 
class Affine:
  def __init__(self, W, b):
    self.params = [W, b]
    self.grads = [np.zeros_like(W), np.zeros_like(b)]
    self.x = None

  def forward(self, x):
    W, b = self.params
    out = np.dot(x, W) + b
    self.x = x
    return out

  def backward(self, dout):
    W, b = self.params
    dx = np.dot(dout, W.T)
    dW = np.dot(self.x.T, dout)
    db = np.sum(dout, axis=0)

    self.grads[0][...] = dW
    self.grads[1][...] = db
    return dx
```
* Softmax with Loss 계층
  * ![softmax](images/fig%201-30.png)
### 1.3.6 가중치 갱신
* 1단계 미니배치
* 2단계 기울기 계산
* 3단계 매개변수 갱신
  * 확률적경사하강법(SGD) 등 다양한 방법이 존재함
* 4단계 반복
* SGD 구현
``` 
class SGD:
  def __init__(Self, lr=0.01):
    self.lr = lr
  
  def update(self,params,grads):
    for i in range(len(params)):
      params[i] -= self.lr * grads[i]
```
* SGD 활용
``` 
model = TwoLayerNet(...)
optimizer = SGD()

for i in range(10000):
  ...
  x_batch, t_batch = get_min_batch(...) # 미니배치 획득
  loss = model.forward(x_batch, t_batch)
  model.backward()
  optimizer.update(modex.params, model.grads)
  ...
```

## 1.4 신경망으로 문제를 풀다
### 1.4.1 스파이럴 데이터셋
* 스파이럴 데이터셋
  * ![스파이럴 데이터셋](images/fig%201-31.png)
### 1.4.2 신경망 구현
* 구현
``` 
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size):
        I, H, O = input_size, hidden_size, output_size

        # 가중치와 편향 초기화
        W1 = 0.01 * np.random.randn(I, H)
        b1 = np.zeros(H)
        W2 = 0.01 * np.random.randn(H, O)
        b2 = np.zeros(O)

        # 계층 생성
        self.layers = [
            Affine(W1, b1),
            Sigmoid(),
            Affine(W2, b2)
        ]
        self.loss_layer = SoftmaxWithLoss()

        # 모든 가중치와 기울기를 리스트에 모은다.
        self.params, self.grads = [], []
        for layer in self.layers:
            self.params += layer.params
            self.grads += layer.grads

    def predict(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return x

    def forward(self, x, t):
        score = self.predict(x)
        loss = self.loss_layer.forward(score, t)
        return loss

    def backward(self, dout=1):
        dout = self.loss_layer.backward(dout)
        for layer in reversed(self.layers):
            dout = layer.backward(dout)
        return dout
```
### 1.4.3 학습용 코드
* 구현
``` 
# 하이퍼파라미터 설정
max_epoch = 300
batch_size = 30
hidden_size = 10
learning_rate = 1.0

# 데이터 읽기, 모델과 옵티마이저 생성
x, t = spiral.load_data()
model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)
optimizer = SGD(lr=learning_rate)

# 학습에 사용하는 변수
data_size = len(x)
max_iters = data_size // batch_size
total_loss = 0
loss_count = 0
loss_list = []

for epoch in range(max_epoch):
    # 데이터 뒤섞기
    idx = np.random.permutation(data_size)
    x = x[idx]
    t = t[idx]

    for iters in range(max_iters):
        batch_x = x[iters*batch_size:(iters+1)*batch_size]
        batch_t = t[iters*batch_size:(iters+1)*batch_size]

        # 기울기를 구해 매개변수 갱신
        loss = model.forward(batch_x, batch_t)
        model.backward()
        optimizer.update(model.params, model.grads)

        total_loss += loss
        loss_count += 1

        # 정기적으로 학습 경과 출력
        if (iters+1) % 10 == 0:
            avg_loss = total_loss / loss_count
            print('| 에폭 %d |  반복 %d / %d | 손실 %.2f'
                  % (epoch + 1, iters + 1, max_iters, avg_loss))
            loss_list.append(avg_loss)
            total_loss, loss_count = 0, 0
```
### 1.4.4 Trainer 클래스
* 활용 예제
``` 
model = TwoLayerNet(...)
optimizer = SGD(lr=1.0)
trainer = Trainer(model, optimizer)
trainer.fit(x, t, max_epoch, batch_size, eval_interval=10)
trainer.plot()
```

## 1.5 계산 고속화
### 1.5.1 비트 정밀도
* 계산 속도 측면에서 32비트가 64비트보다 빠름
### 1.5.2 GPU(쿠파이)
* GPU를 이용해 병렬 계산을 수행해주는 라이브러리
* 엔비디아의 GPU에서만 동작
* CUDA 설치 필요

## 1.6 정리
* 신경망은 입력층, 은닉층(중간층), 출력층을 지닌다.
* 완전연결계층에 의해 선형 변환이 이뤄지고, 활성화 함수에 의해 비선형 변환이 이뤄진다.
* 완전연결계층이나 미니배치 처리는 행렬로 모아 한꺼번에 계산할 수 있다.
* 오차역전파법을 사용하여 신경망의 손실에 관한 기울기를 효율적으로 구할 수 있다.
* 신경망이 수행하는 처리는 계산 그래프로 시각화할 수 있으며, 순전파와 역전파를 이해하는데 도움이 된다.
* 신경망의 구성요소들을 '계층'으로 모듈화해두면, 이를 조립하여 신경망을 쉽게 구성할 수 있다.
* 신경망 고속화에는 GPU를 이용한 병렬 계산과 데이터의 비트 정밀도가 중요하다.