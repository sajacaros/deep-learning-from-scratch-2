# CHAPTER 2 자연어와 단어의 분산 표현

## 2.1 자연어 처리란
### 2.1.1 단어의 의미
* 단어의 의미 표현 방법
  * 시소러스를 활용한 기법
  * 통계 기반 기법
  * 추론 기반 기법(word2vec)(3장 참고)

## 2.2. 시소러스
* 시소러스란?
  * 유의어 사전
  * ![시소러스 예 참고](images/fig%202-2.png)
### 2.2.1 WordNet
* WordNet
  * 가장 유명한 시소러스
  * 프린스턴 대학교에서 1985년부터 구축
### 2.2.2 시소러스의 문제점
* 시대 변화에 대응하기 어렵다.
* 사람을 쓰는 비용은 크다.
  * 현존하는 영어단어 1000만개 대비 WordNet에 등록된 단어는 20만개
* 단어의 미묘한 차이를 표현할 수 없다.

## 2.3 통계 기반 기법
* 말뭉치란?
  * 자연어 처리연구를 위해 수집된 대량의 텍스트 데이터
### 2.3.1 파이썬으로 말뭉치 전처리하기
* word_to_id, id_to_word, corpus 구하기
``` 
import numpy as np

def preprocess(text):
    text = text.lower()
    text = text.replace('.', ' .')
    words = text.split(' ')

    word_to_id = {}
    id_to_word = {}
    for word in words:
        if word not in word_to_id:
            new_id = len(word_to_id)
            word_to_id[word] = new_id
            id_to_word[new_id] = word

    corpus = np.array([word_to_id[w] for w in words])

    return corpus, word_to_id, id_to_word

text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
```
### 2.3.2 단어의 분산 표현
* 단어의 분산 표현이란?
  * distributional representation
  * '단어의 의미'를 정확하게 파악할 수 있는 백터 표현
### 2.3.3 분포 가설
* 분포 가설이란?
  * distributional hypothesis
  * 단어의 의미는 주변 단어에 의해 형성
  * 단어 자체에는 의미가 없고, 그 단어가 사용된 '맥락'이 의미를 형성함
  * ![분포가설](images/fig%202-3.png)
* 맥락이란?
  * 특정 단어를 중심에 둔 그 주변 단어를 말함
### 2.3.4 동시발생 행렬
* 통계 기반 기법
  * statistical based
  * 어떤 단어 주변에 어떤 단어가 몇번 등장하는지를 세어 집계하는 방법
  * ![통계기반기법](images/fig%202-7.png)
``` 
def create_co_matrix(corpus, vocab_size, window_size=1):
  corpus_size = len(corpus)
  co_matrix = np.zeros((vocab_size,vocab_size), dtype=np.int32)
  
  for idx, word_id in enumerate(corpus):
    for i in range(1, window_size+1):
      left_idx = idx - i
      right_idx = idx + i
      if left_idx >= 0:
        left_word_id = corpus[left_idx]
        co_matrix[word_id, left_word_id] += 1
      if right_idx < corpus_size:
        right_word_id = corpus[right_idx]
        co_matrix[word_id, right_word_id] += 1
  return co_matrix

co_matrix = create_co_matrix(corpus, len(id_to_word))
```
### 2.3.5 벡터 간 유사도
* 코사인 유사도
  * cosine similarity
  * 백테 사이의 유사도를 측정하는 방법
  * ![코사인 유사도](images/e%202-1.png)
``` 
def cos_similarity(x, y, eps=1e-8):
  nx = x / (np.sqrt(np.sum(x**2)) + eps)
  ny = y / (np.sqrt(np.sum(y**2)) + eps)
  return np.dot(nx, ny)
```
* 'you'와 'i'의 유사도를 구하는 예제
``` 
text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
vocab_size = len(word_to_id)
C = create_co_matrix(corpus, vocab_size)

c0 = C[word_to_id['you']]
c1 = C[word_to_id['i']]
print(C)
print(cos_similarity(c0, c1))
```
### 2.3.6 유사 단어의 랭킹 표시
* 특정 단어와 관계깊은 단어 출력하는 함수 구현
``` 
def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):
  if query not in word_to_id:
    print(f"{query} 가 존재하지 않음")
    return
  print(f"[query] : {query}")
  query_id = word_to_id[query]
  query_vec = word_matrix[query_id]
  
  vocab_size = len(id_to_word)
  similarity = np.zeros(vocab_size)
  for i in range(vocab_size):
    similarity[i] = cos_similarity(word_matrix[i], query_vec)
  count = 0
  for i in (-1 * similarity).argsort():
    if id_to_word[i] == query:
      continue
    print(f"{id_to_word[i]}: {similarity[i]}")
    
    count+=1
    if count >= top:
      return
most_similar('you', word_to_id, id_to_word, C, top=5)
```

## 2.4 통계 기반 기법 개선하기
### 2.4.1 상호정보량
* 동시발생 행렬의 문제점
  * '발생 횟수'라는 특징은 고빈도 단어에 유리함
  * the 같은 조사에 고빈도 단어가 측정되어 다른 단어와의 강한 관련성을 갖은
  * 점별 상호정보량 척도를 사용하여 이 문제를 해결
* 점멸 상호정보량(PMI)이란?
  * Pointwise Mutual Information
  * ![PMI](images/e%202-2.png)
  * ![PMI](images/e%202-3.png)
  * ex> 
    * 말뭉치 단어 수 10000개
    * the 등장수 1000번
    * car 등장 수 : 20번
    * drive 등장수 : 10번
    * the, car 동시 발생 횟수 : 10번
    * car, drive 동시 발생 횟수 : 5번
    * "the", "car" PMI
      * ![PMI](images/e%202-4.png)
    * "car", "drive" PMI
      * ![PMI](images/e%202-5.png)
* 양의 상호정보량(PPMI)
  * ![PPMI](images/e%202-6.png)
``` 
def ppmi(C, verbose=False, eps=1e-8):
  M = np.zeros_like(C, dtype=np.float32)
  N = np.sum(C)
  S = np.sum(C, axis=0)
  total = C.shape[0] * C.shape[1]
  cnt = 0
  
  for i in range(C.shape[0]):
    for j in range(C.shape[1]):
      pmi = np.log2(C[i,j]*N / (S[j]*S[i])+eps)
      M[i, j] = max(0, pmi)
      
      if verbose:
        cnt += 1
        if cnt % (total//100) == 0:
          print("%.1f%% completed" % (100*cnt/total))
  return M

W = ppmi(C)
print(f"PPMI : {W}")
```
### 2.4.2 차원 감소
* PPMI 행렬의 문제점
  * 대부분 0값임
  * 각 원소의 '중요도'가 낮음
  * 해결방법 : 차원 감소
* 차원 감소란?
  * dimensionality reduction
  * '중요한 정보'는 최대한 유지하면서 벡터의 차원을 줄이는 방법
  * ![차원감소](images/fig%202-8.png)
### 2.4.3 SVD에 의한 차원 감소
* 특잇값분해(SVD)
  * Singular Value Decomposition
  * 차원을 감소시키는 방법
  * ![SVD](images/e%202-7.png)
  * ![SVd](images/fig%202-9.png)
* 특이값이란?
  * '해당 축'의 중요도
  * ![SVD](images/fig%202-10.png)
* 구현
  * 넘파이의 linalg 모듈에서 svd 메서드 제공
``` 
U, S, V = np.linalg.svd(W)
# 2차원으로 줄이기
print(U[0,:2])

import matplotlib.pyplot as plt
for word, word_id in word_to_id.items():
  plt.annotate(word, (U[word_id, 0], U[word_id, 1]))

plt.scatter(U[:, 0], U[:, 1], alpha=0.5)
plt.show()
```
### 2.4.4 PTB 데이터셋
* 펜 트리뱅크
  * Penn Treebank
  * word2vec의 발명자인 토마스 미콜로프가 제공
  * 전처리된 말뭉치 제공
``` 
from dataset import ptb

corpus, word_to_id, id_to_word = ptb.load_data('train')

print(f"말뭉치 크기 : {len(corpus)}")
```
### 2.4.5 PTB 데이터셋 평가
* 구현
``` 
window_size = 2
wordvec_size = 100

corpus, word_to_id, id_to_word = ptb.load_data('train')
vocab_size = len(word_to_id)
print('동시발생 수 계산 ...')
C = create_co_matrix(corpus, vocab_size, window_size)
print('PPMI 계산 ...')
W = ppmi(C, verbose=True)

print('calculating SVD ...')
try:
    # truncated SVD (빠르다!)
    from sklearn.utils.extmath import randomized_svd
    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,
                             random_state=None)
except ImportError:
    # SVD (느리다)
    U, S, V = np.linalg.svd(W)

word_vecs = U[:, :wordvec_size]

querys = ['you', 'year', 'car', 'toyota']
for query in querys:
    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)
```

## 2.5 정리
* WordNet 등의 시소러스를 이용하면 유의어를 얻거나 단어 사이의 유사도를 측정하는 등 유용한 작업을 할 수 있다.
* 시소러스 기반 기법은 시소러스를 작성하는 데 엄청난 인적 자원이 든다거나 새로운 단어에 대응하기 어렵다는 문제가 있다.
* 현재는 말뭉치를 이용해 단어를 벡터화하는 방식이 주로 쓰인다.
* 최근의 단어 벡터화 기법들은 대부분 '단어의 의미는 주변 단어에 의해 형성된다'는 분포 가설에 기초한다.
* 통계 기반 기법은 말뭉치 안의 각 단어에 대해서 그 단어의 주변 단어의 빈도를 집계한다.(동시발생 행렬)
* 동시발생 행렬을 PPMI 행렬로 변환하고 다시 차원을 감소시킴으로써, 거대한 '희소벡터'를 작은 '밀집벡터'로 변환할 수 있다.
* 단어의 벡터 공간에서는 의미가 가까운 단어는 그 거리도 가까울 것으로 기대된다.